{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXAw9uCfwgVPZlCQ+QSO06",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheRealMatri/SEO-Testful/blob/main/Testful_for_Github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries for Hugging Face Transformers, Datasets, JAX, and JAXLib\n",
        "!pip install transformers datasets jax jaxlib"
      ],
      "metadata": {
        "id": "YCt9ZcOViWCB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check installed version of Hugging Face transformers\n",
        "!pip show transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xxn6gUv1pMba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If needed, upgrade transformers to the latest version\n",
        "!pip install --upgrade transformers datasets jax jaxlib"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cPYQ34mWpO6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for model training\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "18Bpps1Qiu0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('ai-forever/rugpt3small_based_on_gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('ai-forever/rugpt3small_based_on_gpt2', torch_dtype=torch.float32)"
      ],
      "metadata": {
        "id": "W_K684KYrtst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the train dataset (with 'train' split)\n",
        "train_dataset = load_dataset('json', data_files='/content/dataset_train.jsonl', split='train')\n",
        "\n",
        "# Load the test dataset (with 'test' split, but no split argument needed)\n",
        "test_dataset = load_dataset('json', data_files='/content/dataset_test.jsonl', split='train')  # Use 'train' here as split name\n"
      ],
      "metadata": {
        "id": "cR9-2Y4Ji_Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a label mapping with all possible categories\n",
        "label_map = {\n",
        "    \"Дети\": 0,\n",
        "    \"Локализация\": 1,\n",
        "    \"Эстетика\": 2,\n",
        "    \"Акции\": 3,\n",
        "    \"Пенсионеры\": 4,\n",
        "    \"Технологии\": 5,\n",
        "    \"Ортодонтия\": 6,\n",
        "    \"Экстренная помощь\": 7,\n",
        "}\n",
        "\n",
        "# Function to encode labels\n",
        "def encode_labels(example):\n",
        "    example['label'] = label_map.get(example['label'], -1)  # Default to -1 if label is unknown\n",
        "    return example\n",
        "\n",
        "# Apply label encoding to both datasets\n",
        "train_dataset = train_dataset.map(encode_labels)\n",
        "test_dataset = test_dataset.map(encode_labels)\n",
        "\n",
        "# Check the updated datasets (optional print)\n",
        "print(train_dataset[0])  # Print the first item in the train dataset\n",
        "print(test_dataset[0])   # Print the first item in the test dataset\n"
      ],
      "metadata": {
        "id": "zgYRWf7ktGiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load model & tokenizer from your saved folder\n",
        "model = GPT2LMHeadModel.from_pretrained('./fine_tuned_model')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_model')\n"
      ],
      "metadata": {
        "id": "nN1MmQWMxT0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to tokenize the text\n",
        "def tokenize_function(examples):\n",
        "    # Concatenate the prompt and completion with a separator (e.g., a special token or just a space)\n",
        "    inputs = [prompt + \"\\n\" + completion for prompt, completion in zip(examples['prompt'], examples['completion'])]\n",
        "\n",
        "    # Tokenize the inputs and ensure the correct format for labels\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Ensure labels are shifted by one token for causal language modeling\n",
        "    model_inputs['labels'] = model_inputs.input_ids.detach().clone()\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the tokenization to the dataset\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "CkaSvfS7jUYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments optimized for GPU (14GB VRAM, 12GB RAM)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',                # Output directory to save model and checkpoints\n",
        "    logging_dir='./logs',                  # Directory for storing logs\n",
        "    logging_steps=500,                     # Log every 500 steps\n",
        "    save_steps=5000,                       # Save checkpoints every 5000 steps\n",
        "    save_total_limit=3,                    # Only keep the last 3 checkpoints\n",
        "    per_device_train_batch_size=48,         # Set batch size for training based on GPU RAM\n",
        "    per_device_eval_batch_size=48,          # Set batch size for evaluation based on GPU RAM         # Accumulate gradients over 4 steps to reduce memory usage\n",
        "    weight_decay=0.01,                     # Strength of weight decay\n",
        "    num_train_epochs=3,                    # Number of epochs\n",
        "    warmup_steps=500,                      # Number of warmup steps for learning rate scheduler\n",
        "    logging_first_step=True,               # Log the first step of training\n",
        "    load_best_model_at_end=True,           # Load the best model at the end of training\n",
        "    metric_for_best_model='loss',     # Metric for selecting the best model\n",
        "    save_strategy=\"steps\",                 # Save the model every certain number of steps\n",
        "    eval_strategy=\"steps\",                 # Evaluate every certain number of steps\n",
        "    eval_steps=200,                        # Evaluate every 200 steps\n",
        "    dataloader_num_workers=2,              # Number of workers for loading data\n",
        "    fp16=True,                             # Enable mixed precision training for faster training\n",
        "    learning_rate=5e-5,                    # Set the learning rate\n",
        "    report_to=\"wandb\",                     # Log metrics to W&B (optional, remove if not needed)\n",
        ")"
      ],
      "metadata": {
        "id": "B7_GbFn6kzgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n"
      ],
      "metadata": {
        "id": "zXcnjsvxk5h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from threading import Thread\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Set your desired training time in seconds (e.g., 1 hour)\n",
        "TRAINING_TIME_LIMIT = 60*10  # 1 hour\n",
        "\n",
        "# Flag to control training\n",
        "stop_training = False\n",
        "\n",
        "# Timer function\n",
        "def stop_training_after_delay():\n",
        "    global stop_training\n",
        "    time.sleep(TRAINING_TIME_LIMIT)\n",
        "    stop_training = True\n",
        "    clear_output()\n",
        "    print(f\"⏰ Time's up! Stopping training after {TRAINING_TIME_LIMIT // 60} minutes.\")\n",
        "\n",
        "# Start timer\n",
        "timer_thread = Thread(target=stop_training_after_delay)\n",
        "timer_thread.start()\n",
        "\n",
        "# Begin training loop\n",
        "while not stop_training:\n",
        "    trainer.train(resume_from_checkpoint=True)\n",
        "    trainer.save_model(\"./fine_tuned_model\")\n",
        "\n",
        "print(\"✅ Training session ended.\")\n"
      ],
      "metadata": {
        "id": "hbtzK5FWlBVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print the evaluation results\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "lBomEtHHtT7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")"
      ],
      "metadata": {
        "id": "AzpAkinBlD5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"./fine_tuned_model\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Create a text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Input your prompt here\n",
        "prompt = \"Сгенерируй рекламу для стамоталогии Smile предлагающей очистку зубов со скидкой 20%\"\n",
        "\n",
        "# Generate output\n",
        "output = generator(prompt, max_length=100, do_sample=True, top_k=50, top_p=0.95, temperature=0.8)\n",
        "\n",
        "# Print generated text\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "-uIC1gMMwXcd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}